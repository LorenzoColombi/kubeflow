{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proviamo a farle in v2\n",
    "\n",
    "#from tensorflow import keras\n",
    "#from minio import Minio\n",
    "#import numpy as np\n",
    "#import json\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "import os\n",
    "from kfp.dsl import Input, Output, Dataset, Model, Metrics, ClassificationMetrics, Artifact\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "@dsl.component(base_image=\"tensorflow/tensorflow\")\n",
    "def load_dataset(x_train_artifact: Output[Dataset], x_test_artifact: Output[Dataset],y_train_artifact: Output[Dataset],y_test_artifact: Output[Dataset]):\n",
    "    '''\n",
    "    get dataset from Keras and load it separating input from output and train from test\n",
    "    '''\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    import os\n",
    "    \n",
    "   \n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    np.save(\"/tmp/x_train.npy\",x_train)\n",
    "    os.rename(\"/tmp/x_train.npy\", x_train_artifact.path)\n",
    "    \n",
    "    np.save(\"/tmp/y_train.npy\",y_train)\n",
    "    os.rename(\"/tmp/y_train.npy\", y_train_artifact.path)\n",
    "    \n",
    "    np.save(\"/tmp/x_test.npy\",x_test)\n",
    "    os.rename(\"/tmp/x_test.npy\", x_test_artifact.path)\n",
    "    \n",
    "    np.save(\"/tmp/y_test.npy\",y_test)\n",
    "    os.rename(\"/tmp/y_test.npy\", y_test_artifact.path)\n",
    "\n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "    print(f\"x_test shape: {x_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    print(\"Success!\")\n",
    "\n",
    "@dsl.component(packages_to_install=['numpy'])\n",
    "def preprocessing(metrics : Output[Metrics], x_train_processed : Output[Dataset], x_test_processed: Output[Dataset],\n",
    "                  x_train_artifact: Input[Dataset], x_test_artifact: Input[Dataset]):\n",
    "    ''' \n",
    "    just reshape and normalize data\n",
    "    '''\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    # load data artifact store\n",
    "    x_train = np.load(x_train_artifact.path) \n",
    "    x_test = np.load(x_test_artifact.path)\n",
    "    \n",
    "    # reshaping the data\n",
    "    # reshaping pixels in a 28x28px image with greyscale, canal = 1. This is needed for the Keras API\n",
    "    x_train = x_train.reshape(-1,28,28,1)\n",
    "    x_test = x_test.reshape(-1,28,28,1)\n",
    "    # normalizing the data\n",
    "    # each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "    \n",
    "    #logging metrics using Kubeflow Artifacts\n",
    "    metrics.log_metric(\"Len x_train\", x_train.shape[0])\n",
    "    metrics.log_metric(\"Len y_train\", x_test.shape[0])\n",
    "   \n",
    "    \n",
    "    # save feuture in artifact store\n",
    "    np.save(\"tmp/x_train.npy\",x_train)\n",
    "    os.rename(\"tmp/x_train.npy\", x_train_processed.path)\n",
    "    \n",
    "    np.save(\"tmp/x_test.npy\",x_test)\n",
    "    os.rename(\"tmp/x_test.npy\", x_test_processed.path)\n",
    "    \n",
    "\n",
    "@dsl.component(base_image=\"tensorflow/tensorflow\")\n",
    "def model_building(ml_model : Output[Model]):\n",
    "    '''\n",
    "    Define the model and load it (not yet compiled to minio)\n",
    "    This way it's more simple to change the model architecture and all the steps and indipendent\n",
    "    '''\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    \n",
    "    #model definition\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    summary = model.summary()\n",
    "    \n",
    "    #saving model\n",
    "    model.save(ml_model.path)\n",
    "    #os.rename(\"/tmp/model.keras\", ml_model.path)\n",
    "   \n",
    "    print(\"Success\")\n",
    "    \n",
    "@dsl.component(base_image=\"tensorflow/tensorflow\", packages_to_install=['scikit-learn'])\n",
    "def model_training(\n",
    "    ml_model : Input[Model],\n",
    "    x_train_processed : Input[Dataset], x_test_processed: Input[Dataset],\n",
    "    y_train_artifact : Input[Dataset], y_test_artifact :Input[Dataset],\n",
    "    hyperparameters : dict, \n",
    "    metrics: Output[Metrics], classification_metrics: Output[ClassificationMetrics], model_trained: Output[Model]\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Build the model with Keras API\n",
    "    Export model metrics\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import glob\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    #load dataset\n",
    "    x_train = np.load(x_train_processed.path)\n",
    "    x_test = np.load(x_test_processed.path)\n",
    "    y_train = np.load(y_train_artifact.path)\n",
    "    y_test = np.load(y_test_artifact.path)\n",
    "    \n",
    "    #load model structure\n",
    "    model = keras.models.load_model(ml_model.path)\n",
    "    \n",
    "    #reading best hyperparameters from katib\n",
    "    lr=float(hyperparameters[\"lr\"])\n",
    "    no_epochs = int(hyperparameters[\"num_epochs\"])\n",
    "    \n",
    "    #compile the model - we want to have a binary outcome\n",
    "    model.compile(tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    #fit the model and return the history while training\n",
    "    history = model.fit(\n",
    "      x=x_train,\n",
    "      y=y_train,\n",
    "      epochs=no_epochs,\n",
    "      batch_size=20,\n",
    "    )\n",
    "\n",
    "     \n",
    "    # Test the model against the test dataset\n",
    "    # Returns the loss value & metrics values for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "    \n",
    "    #build a confusione matrix\n",
    "    y_predict = model.predict(x=x_test)\n",
    "    y_predict = np.argmax(y_predict, axis=1)\n",
    "    cmatrix = confusion_matrix(y_test, y_predict)\n",
    "    cmatrix = cmatrix.tolist()\n",
    "    numbers_list = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    #log confusione matrix\n",
    "    classification_metrics.log_confusion_matrix(numbers_list,cmatrix)\n",
    "  \n",
    "    #Kubeflox metrics export\n",
    "    metrics.log_metric(\"Test loss\", model_loss)\n",
    "    metrics.log_metric(\"Test accuracy\", model_accuracy)\n",
    "    \n",
    "    #adding /1/ subfolder for TFServing and saving model to artifact store\n",
    "    model_trained.uri = model_trained.uri + '/1/'\n",
    "    keras.models.save_model(model,model_trained.path)\n",
    "    \n",
    "@dsl.component(packages_to_install=['kserve','kubernetes'])\n",
    "def model_serving(model_trained : Input[Model]):\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    import os\n",
    "    \n",
    "    #get model uri\n",
    "    uri = model_trained.uri\n",
    "    #replace minio with s3\n",
    "    uri = uri.replace(\"minio\",\"s3\")\n",
    "    #removing last subfolder /1/\n",
    "    uri = uri.rsplit(\"/\",2)[0]\n",
    "    \n",
    "    #TFServing wants this type of structure ./models/1/model\n",
    "    # the number represent the model version\n",
    "    # in this example we use only 1 version\n",
    "    \n",
    "    #Inference server config\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    now = datetime.now()\n",
    "    name=\"digits-recognizer\"\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       service_account_name=\"sa-minio-kserve\",\n",
    "                                       tensorflow=(V1beta1TFServingSpec(\n",
    "                                           storage_uri=uri))))\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    \n",
    "    #replace old inference service with a new one\n",
    "    try:\n",
    "        KServe.delete(name=name, namespace=namespace)\n",
    "        print(\"Modello precedente eliminato\")\n",
    "    except:\n",
    "        print(\"Non posso eliminare\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    KServe.create(isvc)\n",
    "\n",
    " \n",
    "@dsl.pipeline(\n",
    "    name='tutorial-dev',\n",
    "    description='Detect digits')\n",
    "def mnist_pipeline(hyperparameters: dict):\n",
    "    load_task = load_dataset()\n",
    "    preprocess_task = preprocessing(\n",
    "        x_train_artifact = load_task.outputs[\"x_train_artifact\"],\n",
    "        x_test_artifact = load_task.outputs[\"x_test_artifact\"]\n",
    "    )\n",
    "    model_building_task = model_building()\n",
    "    training_task = model_training(\n",
    "        ml_model = model_building_task.outputs[\"ml_model\"],\n",
    "        x_train_processed = preprocess_task.outputs[\"x_train_processed\"],\n",
    "        x_test_processed = preprocess_task.outputs[\"x_test_processed\"],\n",
    "        y_train_artifact = load_task.outputs[\"y_train_artifact\"],\n",
    "        y_test_artifact = load_task.outputs[\"y_test_artifact\"],\n",
    "        hyperparameters = hyperparameters\n",
    "    )\n",
    "    serving_task = model_serving(model_trained = training_task.outputs[\"model_trained\"])\n",
    "    \n",
    " #reading best hyperparameters from katib\n",
    "\n",
    "\n",
    "\n",
    "with open(os.environ['KF_PIPELINES_SA_TOKEN_PATH'], \"r\") as f:\n",
    "    TOKEN = f.read()\n",
    "\n",
    "hyperparameters ={\"hyperparameters\" :  {\"lr\":0.1, \"num_epochs\":1 } }\n",
    "client = kfp.Client(\n",
    "    existing_token=TOKEN,\n",
    "    host='http://ml-pipeline.kubeflow.svc.cluster.local:8888',\n",
    ")\n",
    "namespace=\"kubeflow-user-example-com\"\n",
    "\n",
    "#client.upload_pipeline_version(pipeline_package_path='./mnist-pipeline.yaml',pipeline_name=\"medium tutorial\",pipeline_version_name= \"1.0\")\n",
    "kfp.compiler.Compiler().compile(pipeline_func=mnist_pipeline,package_path='./mnist-classifier.yaml')\n",
    "#client.upload_pipeline(pipeline_package_path='./mnist-classifier.yaml',pipeline_name=\"MNIST classifier tutorial\",namespace = namespace)\n",
    "\n",
    "\n",
    "client.create_run_from_pipeline_func(mnist_pipeline, arguments=hyperparameters,experiment_name=\"test\",namespace=\"kubeflow-user-example-com\",enable_caching=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
