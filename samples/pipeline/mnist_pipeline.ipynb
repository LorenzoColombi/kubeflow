{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proviamo a farle in v2\n",
    "\n",
    "from tensorflow import keras\n",
    "from minio import Minio\n",
    "import numpy as np\n",
    "import json\n",
    "#import kfp.components as components\n",
    "from typing import NamedTuple\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "import os\n",
    "\n",
    "\n",
    "@dsl.component(base_image=\"tensorflow/tensorflow\", packages_to_install=['minio'])\n",
    "def get_data_batch() -> NamedTuple('Outputs', [('datapoints_training_len', float),('datapoints_test_len', float),('dataset_version', str)]):\n",
    "    '''\n",
    "    get dataset from minio and load it to minio separating X from Y and train from test\n",
    "    \n",
    "    Returns: number of example in training dataset, number of example in test dataset, dataset version.\n",
    "    '''\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"10.152.183.148:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "   \n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # save to numpy file, store in Minio\n",
    "    np.save(\"tmp/x_train.npy\",x_train)\n",
    "    minio_client.fput_object(minio_bucket,\"x_train\",\"tmp/x_train.npy\")\n",
    "\n",
    "    np.save(\"tmp/y_train.npy\",y_train)\n",
    "    minio_client.fput_object(minio_bucket,\"y_train\",\"tmp/y_train.npy\")\n",
    "\n",
    "    np.save(\"tmp/x_test.npy\",x_test)\n",
    "    minio_client.fput_object(minio_bucket,\"x_test\",\"tmp/x_test.npy\")\n",
    "\n",
    "    np.save(\"tmp/y_test.npy\",y_test)\n",
    "    minio_client.fput_object(minio_bucket,\"y_test\",\"tmp/y_test.npy\")\n",
    "\n",
    "    dataset_version = \"1.0\"\n",
    "\n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "    print(f\"x_test shape: {x_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    output_tuple = namedtuple('Outputs', ['datapoints_training_len', 'datapoints_test_len', 'dataset_version'])\n",
    "    \n",
    "    return(output_tuple(x_train.shape[0],x_test.shape[0],dataset_version))\n",
    "\n",
    "@dsl.component(base_image=\"tensorflow/tensorflow\", packages_to_install=['minio'])\n",
    "def reshape_data():\n",
    "    '''\n",
    "    get data from minio and reshape this way: len,28,28 -> len,28,28,1 and load (again) to minio\n",
    "    one channel because it's a grey scale image\n",
    "    '''\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"10.152.183.148:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    print(\"getting data from minio\")\n",
    "    \n",
    "    # load data from minio\n",
    "    minio_client.fget_object(minio_bucket,\"x_train\",\"tmp/x_train.npy\")\n",
    "    x_train = np.load(\"tmp/x_train.npy\") \n",
    "    minio_client.fget_object(minio_bucket,\"x_test\",\"tmp/x_test.npy\")\n",
    "    x_test = np.load(\"tmp/x_test.npy\")\n",
    "    \n",
    "    # reshaping the data\n",
    "    # reshaping pixels in a 28x28px image with greyscale, canal = 1. This is needed for the Keras API\n",
    "    x_train = x_train.reshape(-1,28,28,1)\n",
    "    x_test = x_test.reshape(-1,28,28,1)\n",
    "    # normalizing the data\n",
    "    # each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "\n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"x_test shape: {x_test.shape}\")\n",
    "   \n",
    "    \n",
    "    # save data in minio\n",
    "    np.save(\"tmp/x_train.npy\",x_train)\n",
    "    minio_client.fput_object(minio_bucket,\"x_train\",\"tmp/x_train.npy\")\n",
    "    np.save(\"tmp/x_test.npy\",x_test)\n",
    "    minio_client.fput_object(minio_bucket,\"x_test\",\"tmp/x_test.npy\")\n",
    "    \n",
    "@dsl.component(base_image=\"tensorflow/tensorflow\", packages_to_install=['minio','pandas','numpy'])\n",
    "def model_building(\n",
    "    no_epochs:int = 1,\n",
    "    optimizer: str = \"adam\"\n",
    ") -> NamedTuple('Output', [('mlpipeline_ui_metadata', str),('mlpipeline_metrics', str)]):\n",
    "    \"\"\"\n",
    "    Build the model with Keras API\n",
    "    Export model parameters\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import os\n",
    "    import glob\n",
    "    \n",
    "    print(\"using optimizer:\", optimizer)\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"10.152.183.148:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    print(\"getting data from minio\")\n",
    "    \n",
    "    #model definition\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    summary = model.summary()\n",
    "    \n",
    "    #compile the model - we want to have a binary outcome\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    #load dataset from minio\n",
    "    minio_client.fget_object(minio_bucket,\"x_train\",\"tmp/x_train.npy\")\n",
    "    x_train = np.load(\"tmp/x_train.npy\")\n",
    "    minio_client.fget_object(minio_bucket,\"y_train\",\"tmp/y_train.npy\")\n",
    "    y_train = np.load(\"tmp/y_train.npy\")\n",
    "    minio_client.fget_object(minio_bucket,\"x_test\",\"tmp/x_test.npy\")\n",
    "    x_test = np.load(\"tmp/x_test.npy\") \n",
    "    minio_client.fget_object(minio_bucket,\"y_test\",\"tmp/y_test.npy\")\n",
    "    y_test = np.load(\"tmp/y_test.npy\")\n",
    "    \n",
    "    \n",
    "    #fit the model and return the history while training\n",
    "    history = model.fit(\n",
    "      x=x_train,\n",
    "      y=y_train,\n",
    "      epochs=no_epochs,\n",
    "      batch_size=20,\n",
    "    )\n",
    "    \n",
    "    # Test the model against the test dataset\n",
    "    # Returns the loss value & metrics values for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "    \n",
    "    \n",
    "    # Generates output predictions for the input samples.\n",
    "    test_predictions = model.predict(x=x_test)\n",
    "    \n",
    "    # Returns the indices of the maximum values along an axis.\n",
    "    test_predictions = np.argmax(test_predictions,axis=1) \n",
    "    # the prediction outputs 10 values, we take the index number of the highest value, which is the prediction of the model\n",
    "\n",
    "    # generate confusion matrix\n",
    "    confusion_matrix = tf.math.confusion_matrix(labels=y_test,predictions=test_predictions)\n",
    "    confusion_matrix = confusion_matrix.numpy()\n",
    "    vocab = list(np.unique(y_test))\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(confusion_matrix):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    cm_csv = df_cm.to_csv(header=False, index=False)\n",
    "    summary = model.summary()\n",
    "    \n",
    "    #show model summary - how it looks\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x + \"\\n\"))\n",
    "    metric_model_summary = \"\\n\".join(stringlist)\n",
    "    \n",
    "    metadata = {\n",
    "    \"outputs\": [\n",
    "        {\n",
    "            \"type\": \"confusion_matrix\",\n",
    "            \"format\": \"csv\",\n",
    "            \"schema\": [\n",
    "                {'name': 'target', 'type': 'CATEGORY'},\n",
    "                {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                {'name': 'count', 'type': 'NUMBER'},\n",
    "              ],\n",
    "            \"target_col\" : \"actual\",\n",
    "            \"predicted_col\" : \"predicted\",\n",
    "            \"source\": cm_csv,\n",
    "            \"storage\": \"inline\",\n",
    "            \"labels\": [0,1,2,3,4,5,6,7,8,9]\n",
    "        },\n",
    "        {\n",
    "            'storage': 'inline',\n",
    "            'source': '''# Model Overview\n",
    "            ## Model Summary\n",
    "\n",
    "            ```\n",
    "            {}\n",
    "            ```\n",
    "\n",
    "            ## Model Performance\n",
    "\n",
    "            **Accuracy**: {}\n",
    "            **Loss**: {}\n",
    "\n",
    "            '''.format(stringlist,model_accuracy,model_loss),\n",
    "                            'type': 'markdown',\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'model_accuracy',\n",
    "          'numberValue':  float(model_accuracy),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_loss',\n",
    "          'numberValue':  float(model_loss),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        }]}\n",
    "\n",
    "    #save trained model to minio\n",
    "    keras.models.save_model(model,\"tmp/detect-digits\")\n",
    "\n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(minio_path, local_file[1 + len(local_path):])\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "\n",
    "    upload_local_directory_to_minio(\"tmp/detect-digits\",minio_bucket,\"models/detect-digits/1\") # 1 for version 1\n",
    "\n",
    "    print(\"Saved model to minIO\")\n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "\n",
    "    return output(json.dumps(metadata),json.dumps(metrics))\n",
    "\n",
    "@dsl.component(packages_to_install=['kserve','kubernetes'])\n",
    "def model_serving():\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "\n",
    "    now = datetime.now()\n",
    "    #v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "    #name='digits-recognizer-{}'.format(v)\n",
    "    name=\"digits-recognizer\"\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       service_account_name=\"sa-minio-kserve\",\n",
    "                                       tensorflow=(V1beta1TFServingSpec(\n",
    "                                           storage_uri=\"s3://mlpipeline/models/detect-digits/\"))))\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    #provo a eliminare il deploy se esiste\n",
    "    try:\n",
    "        KServe.delete(name=name, namespace=namespace)\n",
    "        print(\"Modello precedente eliminato\")\n",
    "    except:\n",
    "        print(\"Non posso eliminare\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    KServe.create(isvc)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='digits-recognizer-pipeline',\n",
    "    description='Detect digits'\n",
    ")\n",
    "def output_test(no_epochs : int = 1,optimizer : str = \"adam\"):\n",
    "    step1 = get_data_batch()\n",
    "    step2 = reshape_data()\n",
    "    step3 = model_building(no_epochs = no_epochs,optimizer = optimizer)\n",
    "    \n",
    "    step2.after(step1)\n",
    "    step3.after(step2)\n",
    "    \n",
    "    step4 = model_serving()\n",
    "    step4.after(step3)\n",
    "    \n",
    " \n",
    "#MAIN\n",
    "print(\"start\")\n",
    "\n",
    "arguments = {\"optimizer\":\"adam\",\"no_epochs\":1}\n",
    "\n",
    "with open(os.environ['KF_PIPELINES_SA_TOKEN_PATH'], \"r\") as f:\n",
    "    TOKEN = f.read()\n",
    "\n",
    "client = kfp.Client(\n",
    "    existing_token=TOKEN,\n",
    "    host='http://ml-pipeline.kubeflow.svc.cluster.local:8888',\n",
    ")\n",
    "namespace=\"kubeflow-user-example-com\"\n",
    "kfp.compiler.Compiler().compile(pipeline_func=output_test,package_path='./mnist-pipeline.yaml')\n",
    "\n",
    "#client.create_run_from_pipeline_func(output_test,arguments=arguments,experiment_name=\"test\",namespace=\"kubeflow-user-example-com\")\n",
    "#client.upload_pipeline(pipeline_package_path='output_test.yaml',pipeline_version_name=\"0.4\",pipeline_name=\"mnist video tutorial\")\n",
    "#client.upload_pipeline(pipeline_package_path='./mnist-pipeline.yaml',pipeline_name=\"mnist video tutorial\",namespace = namespace)\n",
    "client.upload_pipeline_version(pipeline_package_path='./mnist-pipeline.yaml',pipeline_name=\"mnist video tutorial\",pipeline_version_name= \"1.5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
